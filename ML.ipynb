{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "\n",
    "\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "\n",
    "def create_model():\n",
    "    def rmse(y_true, y_pred):\n",
    "        from keras import backend\n",
    "        return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "    def r_square(y_true, y_pred):\n",
    "        from keras import backend as K\n",
    "        SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "        SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "        return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,activation = 'sigmoid'))\n",
    "    opt = tf.keras.optimizers.Adam(lr=1e-3,decay=1e-5)\n",
    "\n",
    "    model.compile(loss='mse',optimizer=opt,metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = KerasClassifier(build_fn=create_model,verbose=10)\n",
    "\n",
    "batch_size = [5,10,20,50,100]\n",
    "epochs = [1,5]\n",
    "\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3,verbose=10)\n",
    "grid_result = grid.fit(X_train, y_train,verbose=False)\n",
    "\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error (rmse) for regression\n",
    "def rmse(y_true, y_pred):\n",
    "    from keras import backend\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "def r_square(y_true, y_pred):\n",
    "    from keras import backend as K\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return (1 - SS_res/(SS_tot + K.epsilon()))\n",
    "\n",
    "#Train Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(128,activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "opt = tf.keras.optimizers.Adam(lr=1e-3,decay=1e-5)\n",
    "\n",
    "model.compile(loss='mse',optimizer=opt,metrics=[\"mean_squared_error\", rmse, r_square])\n",
    "history = model.fit(X_train,y_train,epochs=5,batch_size=20,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write out model\n",
    "name = 'test'\n",
    "\n",
    "hist_df = pd.DataFrame(history.history)\n",
    "model_json = model.to_json()\n",
    "with open(\"ML_Models/{}_model.json\".format(name), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"ML_Models/{}_model.h5\".format(name))\n",
    "with open('ML_Models/{}_hist.json'.format(name), 'w') as f:\n",
    "    hist_df.to_json(f)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASS BASED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class test:\n",
    "    def __init__(self,data_path,position_number,**kwargs):\n",
    "        for key,value in kwargs.items():\n",
    "            if key == 'logfile':\n",
    "                self.logfile = value\n",
    "        self.position_number = position_number\n",
    "        self.data_path = data_path\n",
    "    def printer(self):\n",
    "        print(self.logfile)\n",
    "t = test('s',2,logfile='ljk')\n",
    "t.printer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Dataset('s',2,logfile='lkj')\n",
    "t.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import CO2_functions\n",
    "import CO2_Processing\n",
    "import pandas as pd\n",
    "from CO2_functions import *\n",
    "from CO2_Processing import *\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self,data_path,position_number,**kwargs):\n",
    "        for key,value in kwargs.items():\n",
    "            if key == 'logfile':\n",
    "                self.logfile = value\n",
    "        self.position_number = position_number\n",
    "        self.data_path = data_path\n",
    "    \n",
    "    def _get_date_range(self):\n",
    "        if self.position_number == 4:\n",
    "            self.start_date = '2019-09-24'\n",
    "            self.end_date = '2019-10-03'\n",
    "        elif self.position_number == 6:\n",
    "            self.start_date = '2019-11-06'\n",
    "            self.end_date = '2019-11-27'\n",
    "    \n",
    "    def _data_retrieve(self):\n",
    "        first_go = True #flag for first time through\n",
    "        self._get_date_range()\n",
    "        for single_date in daterange(self.start_date,self.end_date): #\"daterange()\" from CO2_processing\n",
    "            if os.path.exists(\"{}/{}.pickle\".format(self.data_path,single_date)):\n",
    "                print_log_flush(\"Retrieving data for {}\".format(single_date),self.logfile)\n",
    "                if first_go: #open and create the data structure\n",
    "                    with open('{}/{}.pickle'.format(self.data_path,single_date), 'rb') as handle:\n",
    "                        data = pickle.load(handle)\n",
    "                    first_go=False\n",
    "                else: #append the next day to the dta structure\n",
    "                    with open('{}/{}.pickle'.format(self.data_path,single_date), 'rb') as handle:\n",
    "                        new_data = pickle.load(handle)\n",
    "                    for key in data:\n",
    "                        if key in new_data.keys():\n",
    "                            data[key] = pd.concat([data[key],new_data[key]])\n",
    "            else: \n",
    "                print_log_flush(\"No data found for {}\".format(single_date),logfile)\n",
    "                print_log_flush(os.listdir(self.data_path),logfile)\n",
    "                continue\n",
    "        return data\n",
    "        \n",
    "    def _preprocess(self):\n",
    "        data = remove_spikes(pd.read_pickle('Spike_ETs.pkl'),self._data_retrieve()) #CO2_Processing\n",
    "        print_log_flush('Removing Impulses',logfile)\n",
    "        print_log_flush('Downsampling and Concatenating',logfile)\n",
    "        data = downsample_and_concatenate(data) #CO2_Processing\n",
    "        \n",
    "        if self.position_number == 4:\n",
    "            data = sept24_26_correction(data) #CO2_Processing\n",
    "            data = combine_vent_data(data,1) #Combine LI_Vent and Vent_Anem_Temp into a single df by sampling rate #CO2_processing\n",
    "            data['Vent_Mass'] = moving_mass_flow(data['Vent_Mass']) #Add the moving mass flow rate based on function developed.  #CO2_processing\n",
    "            data['Vent_Mass'] = pd.concat([\\\n",
    "                                   data['Vent_Mass'].loc[(data['Vent_Mass'].index>'2019-09-24 08:57:00')&\\\n",
    "                                                         (data['Vent_Mass'].index<'2019-09-26 08:00:00')],\\\n",
    "                                   data['Vent_Mass'].loc[(data['Vent_Mass'].index>'2019-09-26 12:00:00')&\\\n",
    "                                                         (data['Vent_Mass'].index<'2019-10-03 13:00:00')].interpolate()])\n",
    "        elif self.position_number == 6:\n",
    "            #Processing for position 6\n",
    "            data = combine_vent_data(data,1) #Combine LI_Vent and Vent_Anem_Temp into a single df by sampling rate \n",
    "            data['Vent_Mass'] = moving_mass_flow(data['Vent_Mass']) #Add the moving mass flow rate based on function developed. \n",
    "            for key in data:\n",
    "                data[key] = pd.concat([\\\n",
    "                               data[key].loc[(data[key].index>'2019-11-06 00:00:00')&(data[key].index<'2019-11-25 12:00:00')],\\\n",
    "                               data[key].loc[(data[key].index>'2019-11-25 17:00:00')&(data[key].index<'2019-11-27 10:28:00')]])\n",
    "\n",
    "            data['Multi'] = data['Multi'].loc[data['Multi']['CO2_3']<600]\n",
    "            data['Multi'] = data['Multi'].loc[data['Multi']['Wind_Velocity']>1.0]\n",
    "\n",
    "        self.data = data\n",
    "        \n",
    "\n",
    "class ML_Data:\n",
    "    def __init__(self,feature_columns,downsample_sec,periods_to_lag,tower,train_percent):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.downsample_sec = downsample_sec\n",
    "        self.periods_to_lag = periods_to_lag\n",
    "        self.tower = tower\n",
    "        self.train_percent = train_percent\n",
    "    \n",
    "    def _prepare_and_downsample(self,data):\n",
    "        #get the correct data from the tower (multi or picarro)\n",
    "        self.position_number = data.position_number\n",
    "        if self.tower == 'Multi':\n",
    "            tower = data.data['Multi']\n",
    "        elif self.tower == 'Picarro':\n",
    "            tower=data.data['Picarro']\n",
    "        else:\n",
    "            raise NameError('tower_id must be a valid tower, either \"Multi\" or \"Picarro\"')\n",
    "        vent=data.data['Vent_Mass']\n",
    "        tower_proc = dwn_sample(tower,self.downsample_sec) #CO2_Processing\n",
    "        vent_proc = dwn_sample(vent,self.downsample_sec) #CO2_Processing\n",
    "        df = pd.concat([tower_proc,vent_proc],axis=1)\n",
    "        #Concatenate and add wind speed & direction if picarro data\n",
    "        if self.tower == 'Picarro':\n",
    "            df = wind_add(df,'ANEM_X','ANEM_Y') #CO2_functions\n",
    "        #Drop columns\n",
    "        if 'm_dot' not in self.feature_columns:\n",
    "            self.feature_columns.append('m_dot')\n",
    "        df = df[self.feature_columns]\n",
    "\n",
    "        #Make mass flux the last column\n",
    "        loc = df.columns.get_loc('m_dot')\n",
    "        cols = df.columns.tolist()\n",
    "        cols = cols[0:loc]+cols[(loc+1):]+[cols[loc]]\n",
    "        df = df[cols]   \n",
    "        \n",
    "        self.df_preprocessed = df\n",
    "    \n",
    "    def _ML_Process(self,data):\n",
    "        if not hasattr(self,'df_preprocessed'):\n",
    "            self._prepare_and_downsample(data)\n",
    "        if (hasattr(data,'logfile')) & (not hasattr(self,'logfile')):\n",
    "            self.logfile = data.logfile\n",
    "        n_periods = self.periods_to_lag#how many periods to lag\n",
    "        n_features = len(self.df_preprocessed.columns)-1#how many features exist in the feature matrix (number of cols - target col)\n",
    "        time_lagged = series_to_supervised(self.df_preprocessed.dropna(),n_in=0,n_out=n_periods,dropnan=False) #CO2_Processing\n",
    "        time_lagged_reframed = delete_unwanted_cols(time_lagged) #delete unneccesary columns #CO2_Processing\n",
    "\n",
    "        \n",
    "        #Make mass flux at t the last column\n",
    "        loc = time_lagged_reframed.columns.get_loc('m_dot(t)')\n",
    "        cols = time_lagged_reframed.columns.tolist()\n",
    "        cols = cols[0:loc]+cols[(loc+1):]+[cols[loc]]\n",
    "        time_lagged_reframed = time_lagged_reframed[cols]\n",
    "        \n",
    "        values = time_lagged_reframed.dropna().values #Convert to numpy for processing\n",
    "        min_max_scalar = preprocessing.MinMaxScaler() #setup scaling\n",
    "        values_scaled = min_max_scalar.fit_transform(values) #scale all values from 0 to 1\n",
    "        self.min_max_scalar = min_max_scalar\n",
    "        \n",
    "        train_size = int(len(values)*self.train_percent) \n",
    "        train = values_scaled[:train_size,:]  #Get train/test arrays\n",
    "        test = values_scaled[train_size:,:]\n",
    "                \n",
    "        X_train,y_train = train[:,:-1], train[:,-1] #Get feature/target arrays\n",
    "        X_test, y_test = test[:,:-1], test[:,-1]\n",
    "        \n",
    "        self.orig_X_train_shape = X_train.shape\n",
    "        self.orig_X_test_shape = X_test.shape\n",
    "        self.orig_y_train_shape = y_train.shape\n",
    "        self.orig_y_test_shape = y_test.shape\n",
    "        \n",
    "        X_train = X_train.reshape((X_train.shape[0], n_periods, n_features)) \n",
    "        X_test = X_test.reshape((X_test.shape[0], n_periods, n_features))\n",
    "        \n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "# import os\n",
    "# os.environ['KERAS_BACKEND'] = 'theano'\n",
    "# import keras\n",
    "\n",
    "\n",
    "import CO2_functions\n",
    "import CO2_Processing\n",
    "import pandas as pd\n",
    "from CO2_functions import *\n",
    "from CO2_Processing import *\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout\n",
    "import os\n",
    "from keras.models import model_from_json\n",
    "import json\n",
    "from keras import backend as K\n",
    "import gc\n",
    "\n",
    "class ML_Model_Builder:\n",
    "    def __init__(self,activation,neurons,dropout_rate,learn_rate,decay,batch_size,epochs):\n",
    "        self.activation = activation\n",
    "        self.neurons = neurons\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learn_rate = learn_rate\n",
    "        self.decay = decay\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def _create_model(self):\n",
    "        def rmse(y_true, y_pred):\n",
    "            return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1))\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(LSTM(self.neurons,activation=self.activation))\n",
    "        model.add(Dropout(self.dropout_rate))\n",
    "        model.add(Dense(1,activation = 'sigmoid'))\n",
    "        \n",
    "        opt = keras.optimizers.Adam(lr=self.learn_rate,decay=self.decay)\n",
    "        self.opt_string = 'tf.keras.optimizers.Adam(lr=self.learn_rate,decay=self.decay)'\n",
    "        \n",
    "        self.loss = 'mse'\n",
    "        \n",
    "        model.compile(loss=self.loss,optimizer=opt,metrics=[rmse])\n",
    "        self.model = model\n",
    "        \n",
    "    def _train_model(self,data):\n",
    "        self.position_number = data.position_number\n",
    "        self.periods_to_lag = data.periods_to_lag\n",
    "        self.downsample_sec = data.downsample_sec\n",
    "        self.feature_columns = data.feature_columns\n",
    "        self.tower = data.tower\n",
    "        self.train_percent = data.train_percent\n",
    "        if (hasattr(data,'logfile')) & (not hasattr(self,'logfile')):\n",
    "            self.logfile = data.logfile\n",
    "        if not hasattr(self,'model'):\n",
    "            self._create_model()\n",
    "        \n",
    "        print_log_flush(f\"Downsampling = {self.downsample_sec}\\nLag Periods = {self.periods_to_lag}\\\n",
    "        \\nactivation={self.activation}\\nneurons={self.neurons}\\ndropout_rate={self.dropout_rate}\\\n",
    "        \\nlearn_rate={self.learn_rate}\\ndecay={self.decay}\\nbatch size={self.batch_size}\\nepochs={self.epochs}\",logfile)\n",
    "        \n",
    "        self.history = self.model.fit(data.X_train,data.y_train,epochs=self.epochs,batch_size=self.batch_size,\\\n",
    "                                      validation_data=(data.X_test,data.y_test),verbose=1)\n",
    "\n",
    "        del self.model\n",
    "        gc.collect()\n",
    "        K.clear_session()\n",
    "        tf.compat.v1.reset_default_graph() # TF graph isn't same as Keras graph\n",
    "        #print(f\"Final RMSE for this model: {self.history.history['rmse'][-1]}\")\n",
    "    \n",
    "    def _del_and_clear(self):\n",
    "        K.clear_session()\n",
    "        #tf.reset_default_graph()\n",
    "        #tf.contrib.keras.backend.clear_session()\n",
    "    \n",
    "    def _fit_data(self,data):\n",
    "        print(f\"Fitting data from X_test\")\n",
    "        data.y_fit = self.model.predict(data.X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MANUAL GRIDSEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from CO2_Dataset_Preparation import *\n",
    "# from ML_Model_Master import *\n",
    "from datetime import datetime\n",
    "\n",
    "position_number = 4\n",
    "feature_columns = ['Pic_CO2','ANEM_X','ANEM_Y','ANEM_Z','wd','ws']\n",
    "downsample_sec = 60\n",
    "periods_to_lag = [1]\n",
    "tower = 'Picarro'\n",
    "train_percent = 0.7\n",
    "\n",
    "activation = 'relu'\n",
    "neurons = [128]\n",
    "dropout_rate = [0.2]\n",
    "learn_rate = [0.001,1e-4,1e-5]\n",
    "decay = [1e-5,1e-6]\n",
    "batch_size = [10,20,50,100]\n",
    "epochs = [10]#,50,100]\n",
    "\n",
    "file_name = 'testlog'#'ML_Models/PN{}_DS{}_Lag{}_Neur{}_DR{}'.format(position_number,downsample_sec,periods_to_lag[0],neurons[0],dropout_rate[0])\n",
    "logfile=open('{}.txt'.format(file_name),'w')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************START TIME = 2020/03/21 14:53:17************************************\n",
      "-------------------------BEGIN: GRIDSEARCH TESTING-------------------------\n",
      "-------------------------PREPROCESSSING-------------------------\n",
      "Retrieving data for 2019-09-24\n",
      "Retrieving data for 2019-09-25\n",
      "Retrieving data for 2019-09-26\n",
      "Retrieving data for 2019-09-27\n",
      "Retrieving data for 2019-09-28\n",
      "Retrieving data for 2019-09-29\n",
      "Retrieving data for 2019-09-30\n",
      "Retrieving data for 2019-10-01\n",
      "Retrieving data for 2019-10-02\n",
      "Retrieving data for 2019-10-03\n",
      "Removing Impulses\n",
      "Downsampling and Concatenating\n",
      "Concatenating Picarro Data\n",
      "Concatenating Multi Data\n",
      "setting night vent data to zero\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "print_log_flush('*******************************START TIME = {}************************************'.format(dt_string),logfile)\n",
    "print_log_flush('-------------------------BEGIN: GRIDSEARCH TESTING-------------------------',logfile)\n",
    "print_log_flush('-------------------------PREPROCESSSING-------------------------',logfile)\n",
    "\n",
    "data = Dataset('../CO2_Data_Final',position_number,logfile=logfile)\n",
    "data._preprocess()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------BUILD AND TRAIN MODELS-------------------------\n",
      "Downsampling by mean at 60 seconds\n",
      "Downsampling by mean at 60 seconds\n",
      "Adding Wind Direction as 'wd'\n",
      "Adding Wind Speed as 'ws'\n",
      "---Training Model: 0 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 9s 949us/step - loss: 0.0585 - rmse: 0.1981 - val_loss: 0.0375 - val_rmse: 0.1535\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 7s 766us/step - loss: 0.0372 - rmse: 0.1474 - val_loss: 0.0296 - val_rmse: 0.1270\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 7s 737us/step - loss: 0.0345 - rmse: 0.1349 - val_loss: 0.0285 - val_rmse: 0.1232\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 8s 840us/step - loss: 0.0339 - rmse: 0.1299 - val_loss: 0.0285 - val_rmse: 0.1199\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 7s 760us/step - loss: 0.0337 - rmse: 0.1277 - val_loss: 0.0277 - val_rmse: 0.1161\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 7s 734us/step - loss: 0.0333 - rmse: 0.1258 - val_loss: 0.0276 - val_rmse: 0.1172\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 7s 793us/step - loss: 0.0331 - rmse: 0.1249 - val_loss: 0.0270 - val_rmse: 0.1168\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 7s 774us/step - loss: 0.0329 - rmse: 0.1238 - val_loss: 0.0276 - val_rmse: 0.1131\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 7s 766us/step - loss: 0.0327 - rmse: 0.1229 - val_loss: 0.0268 - val_rmse: 0.1110\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 7s 812us/step - loss: 0.0323 - rmse: 0.1212 - val_loss: 0.0261 - val_rmse: 0.1080\n",
      "---Training Model: 1 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 6s 623us/step - loss: 0.0695 - rmse: 0.2197 - val_loss: 0.0523 - val_rmse: 0.1892\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 4s 403us/step - loss: 0.0437 - rmse: 0.1695 - val_loss: 0.0351 - val_rmse: 0.1503\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 4s 455us/step - loss: 0.0373 - rmse: 0.1483 - val_loss: 0.0296 - val_rmse: 0.1330\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 3s 373us/step - loss: 0.0355 - rmse: 0.1395 - val_loss: 0.0293 - val_rmse: 0.1268\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 4s 402us/step - loss: 0.0346 - rmse: 0.1350 - val_loss: 0.0291 - val_rmse: 0.1219\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 4s 432us/step - loss: 0.0342 - rmse: 0.1323 - val_loss: 0.0284 - val_rmse: 0.1217\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 4s 429us/step - loss: 0.0338 - rmse: 0.1304 - val_loss: 0.0286 - val_rmse: 0.1188\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 4s 400us/step - loss: 0.0334 - rmse: 0.1288 - val_loss: 0.0278 - val_rmse: 0.1178\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 4s 402us/step - loss: 0.0332 - rmse: 0.1268 - val_loss: 0.0278 - val_rmse: 0.1182\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 3s 379us/step - loss: 0.0331 - rmse: 0.1259 - val_loss: 0.0275 - val_rmse: 0.1145\n",
      "---Training Model: 2 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 3s 331us/step - loss: 0.0952 - rmse: 0.2732 - val_loss: 0.0638 - val_rmse: 0.2101\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 2s 213us/step - loss: 0.0543 - rmse: 0.1849 - val_loss: 0.0565 - val_rmse: 0.1969\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 2s 216us/step - loss: 0.0491 - rmse: 0.1783 - val_loss: 0.0465 - val_rmse: 0.1796\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 2s 170us/step - loss: 0.0432 - rmse: 0.1697 - val_loss: 0.0368 - val_rmse: 0.1558\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 2s 206us/step - loss: 0.0389 - rmse: 0.1561 - val_loss: 0.0316 - val_rmse: 0.1394\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 2s 198us/step - loss: 0.0367 - rmse: 0.1465 - val_loss: 0.0292 - val_rmse: 0.1339\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 2s 205us/step - loss: 0.0356 - rmse: 0.1410 - val_loss: 0.0289 - val_rmse: 0.1283\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 2s 203us/step - loss: 0.0348 - rmse: 0.1362 - val_loss: 0.0292 - val_rmse: 0.1243\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 2s 195us/step - loss: 0.0346 - rmse: 0.1350 - val_loss: 0.0291 - val_rmse: 0.1228\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 2s 196us/step - loss: 0.0342 - rmse: 0.1326 - val_loss: 0.0283 - val_rmse: 0.1218\n",
      "---Training Model: 3 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 2s 271us/step - loss: 0.1268 - rmse: 0.3294 - val_loss: 0.0706 - val_rmse: 0.2472\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 1s 123us/step - loss: 0.0595 - rmse: 0.2067 - val_loss: 0.0629 - val_rmse: 0.2081\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 1s 106us/step - loss: 0.0550 - rmse: 0.1864 - val_loss: 0.0595 - val_rmse: 0.1989\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 1s 126us/step - loss: 0.0522 - rmse: 0.1823 - val_loss: 0.0550 - val_rmse: 0.1915\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 1s 103us/step - loss: 0.0489 - rmse: 0.1796 - val_loss: 0.0487 - val_rmse: 0.1818\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 1s 131us/step - loss: 0.0458 - rmse: 0.1751 - val_loss: 0.0433 - val_rmse: 0.1690\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 1s 89us/step - loss: 0.0426 - rmse: 0.1672 - val_loss: 0.0381 - val_rmse: 0.1587\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 1s 122us/step - loss: 0.0403 - rmse: 0.1604 - val_loss: 0.0354 - val_rmse: 0.1485\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 1s 107us/step - loss: 0.0385 - rmse: 0.1537 - val_loss: 0.0326 - val_rmse: 0.1415\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 1s 133us/step - loss: 0.0375 - rmse: 0.1489 - val_loss: 0.0311 - val_rmse: 0.1357\n",
      "---Training Model: 4 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-06\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 8s 939us/step - loss: 0.0578 - rmse: 0.1967 - val_loss: 0.0375 - val_rmse: 0.1566\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 7s 788us/step - loss: 0.0372 - rmse: 0.1481 - val_loss: 0.0291 - val_rmse: 0.1281\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 7s 821us/step - loss: 0.0347 - rmse: 0.1348 - val_loss: 0.0285 - val_rmse: 0.1239\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 7s 792us/step - loss: 0.0342 - rmse: 0.1315 - val_loss: 0.0281 - val_rmse: 0.1214\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 7s 778us/step - loss: 0.0336 - rmse: 0.1282 - val_loss: 0.0277 - val_rmse: 0.1161\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 8s 861us/step - loss: 0.0330 - rmse: 0.1261 - val_loss: 0.0270 - val_rmse: 0.1169\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 7s 818us/step - loss: 0.0329 - rmse: 0.1244 - val_loss: 0.0275 - val_rmse: 0.1122\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 7s 729us/step - loss: 0.0328 - rmse: 0.1234 - val_loss: 0.0260 - val_rmse: 0.1110\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 6s 704us/step - loss: 0.0324 - rmse: 0.1212 - val_loss: 0.0276 - val_rmse: 0.1096\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8990/8990 [==============================] - 7s 780us/step - loss: 0.0322 - rmse: 0.1206 - val_loss: 0.0266 - val_rmse: 0.1094\n",
      "---Training Model: 5 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-06\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 5s 554us/step - loss: 0.0674 - rmse: 0.2139 - val_loss: 0.0506 - val_rmse: 0.1881\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 4s 423us/step - loss: 0.0425 - rmse: 0.1657 - val_loss: 0.0320 - val_rmse: 0.1439\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 4s 438us/step - loss: 0.0364 - rmse: 0.1447 - val_loss: 0.0302 - val_rmse: 0.1255\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 4s 406us/step - loss: 0.0347 - rmse: 0.1356 - val_loss: 0.0278 - val_rmse: 0.1212\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 4s 412us/step - loss: 0.0341 - rmse: 0.1309 - val_loss: 0.0282 - val_rmse: 0.1191\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 4s 426us/step - loss: 0.0333 - rmse: 0.1280 - val_loss: 0.0271 - val_rmse: 0.1161\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 4s 441us/step - loss: 0.0333 - rmse: 0.1261 - val_loss: 0.0275 - val_rmse: 0.1145\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 4s 397us/step - loss: 0.0332 - rmse: 0.1250 - val_loss: 0.0266 - val_rmse: 0.1125\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 4s 407us/step - loss: 0.0329 - rmse: 0.1240 - val_loss: 0.0269 - val_rmse: 0.1118\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 4s 435us/step - loss: 0.0326 - rmse: 0.1225 - val_loss: 0.0265 - val_rmse: 0.1121\n",
      "---Training Model: 6 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-06\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 3s 335us/step - loss: 0.1005 - rmse: 0.2824 - val_loss: 0.0636 - val_rmse: 0.2152\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 2s 177us/step - loss: 0.0547 - rmse: 0.1873 - val_loss: 0.0572 - val_rmse: 0.1961\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 2s 178us/step - loss: 0.0487 - rmse: 0.1782 - val_loss: 0.0448 - val_rmse: 0.1731\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 2s 214us/step - loss: 0.0418 - rmse: 0.1648 - val_loss: 0.0350 - val_rmse: 0.1497\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 2s 206us/step - loss: 0.0383 - rmse: 0.1533 - val_loss: 0.0318 - val_rmse: 0.1365\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 2s 201us/step - loss: 0.0367 - rmse: 0.1452 - val_loss: 0.0297 - val_rmse: 0.1317\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 2s 186us/step - loss: 0.0357 - rmse: 0.1404 - val_loss: 0.0287 - val_rmse: 0.1276\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 1s 134us/step - loss: 0.0351 - rmse: 0.1373 - val_loss: 0.0287 - val_rmse: 0.1239\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 2s 201us/step - loss: 0.0347 - rmse: 0.1353 - val_loss: 0.0284 - val_rmse: 0.1224\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 2s 205us/step - loss: 0.0343 - rmse: 0.1332 - val_loss: 0.0281 - val_rmse: 0.1223\n",
      "---Training Model: 7 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.001\n",
      "decay=1e-06\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 3s 279us/step - loss: 0.1253 - rmse: 0.3269 - val_loss: 0.0705 - val_rmse: 0.2462\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 1s 98us/step - loss: 0.0595 - rmse: 0.2072 - val_loss: 0.0631 - val_rmse: 0.2076\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 1s 134us/step - loss: 0.0550 - rmse: 0.1851 - val_loss: 0.0596 - val_rmse: 0.1981\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 1s 109us/step - loss: 0.0522 - rmse: 0.1817 - val_loss: 0.0550 - val_rmse: 0.1917\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 1s 132us/step - loss: 0.0492 - rmse: 0.1792 - val_loss: 0.0496 - val_rmse: 0.1838\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 1s 124us/step - loss: 0.0460 - rmse: 0.1751 - val_loss: 0.0431 - val_rmse: 0.1723\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 1s 130us/step - loss: 0.0427 - rmse: 0.1682 - val_loss: 0.0384 - val_rmse: 0.1571\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 1s 139us/step - loss: 0.0400 - rmse: 0.1598 - val_loss: 0.0347 - val_rmse: 0.1459\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 1s 106us/step - loss: 0.0384 - rmse: 0.1536 - val_loss: 0.0320 - val_rmse: 0.1378\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 1s 132us/step - loss: 0.0369 - rmse: 0.1470 - val_loss: 0.0313 - val_rmse: 0.1318\n",
      "---Training Model: 8 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.0001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 8s 883us/step - loss: 0.1293 - rmse: 0.3333 - val_loss: 0.0746 - val_rmse: 0.2569\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 7s 804us/step - loss: 0.0625 - rmse: 0.2218 - val_loss: 0.0628 - val_rmse: 0.2150\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 7s 821us/step - loss: 0.0557 - rmse: 0.1925 - val_loss: 0.0605 - val_rmse: 0.2023\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 7s 771us/step - loss: 0.0534 - rmse: 0.1849 - val_loss: 0.0571 - val_rmse: 0.1961\n",
      "Epoch 5/10\n",
      "8990/8990 [==============================] - 7s 820us/step - loss: 0.0510 - rmse: 0.1821 - val_loss: 0.0524 - val_rmse: 0.1905\n",
      "Epoch 6/10\n",
      "8990/8990 [==============================] - 7s 781us/step - loss: 0.0484 - rmse: 0.1802 - val_loss: 0.0479 - val_rmse: 0.1827\n",
      "Epoch 7/10\n",
      "8990/8990 [==============================] - 7s 748us/step - loss: 0.0462 - rmse: 0.1768 - val_loss: 0.0440 - val_rmse: 0.1749\n",
      "Epoch 8/10\n",
      "8990/8990 [==============================] - 7s 822us/step - loss: 0.0437 - rmse: 0.1722 - val_loss: 0.0407 - val_rmse: 0.1677\n",
      "Epoch 9/10\n",
      "8990/8990 [==============================] - 7s 808us/step - loss: 0.0424 - rmse: 0.1687 - val_loss: 0.0383 - val_rmse: 0.1610\n",
      "Epoch 10/10\n",
      "8990/8990 [==============================] - 7s 765us/step - loss: 0.0413 - rmse: 0.1647 - val_loss: 0.0360 - val_rmse: 0.1561\n",
      "---Training Model: 9 of 23---\n",
      "Downsampling = 60\n",
      "Lag Periods = 1        \n",
      "activation=relu\n",
      "neurons=128\n",
      "dropout_rate=0.2        \n",
      "learn_rate=0.0001\n",
      "decay=1e-05\n",
      "batch size=[10, 20, 50, 100]\n",
      "epochs=[10]\n",
      "Train on 8990 samples, validate on 3853 samples\n",
      "Epoch 1/10\n",
      "8990/8990 [==============================] - 5s 589us/step - loss: 0.1523 - rmse: 0.3632 - val_loss: 0.1044 - val_rmse: 0.2994\n",
      "Epoch 2/10\n",
      "8990/8990 [==============================] - 4s 439us/step - loss: 0.0836 - rmse: 0.2705 - val_loss: 0.0691 - val_rmse: 0.2432\n",
      "Epoch 3/10\n",
      "8990/8990 [==============================] - 4s 412us/step - loss: 0.0614 - rmse: 0.2189 - val_loss: 0.0638 - val_rmse: 0.2198\n",
      "Epoch 4/10\n",
      "8990/8990 [==============================] - 4s 415us/step - loss: 0.0568 - rmse: 0.1983 - val_loss: 0.0620 - val_rmse: 0.2092\n",
      "Epoch 5/10\n",
      "7520/8990 [========================>.....] - ETA: 0s - loss: 0.0551 - rmse: 0.1903"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f802df7878fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m                             \u001b[0mprint_log_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"---Training Model: {i} of {tot_train}---\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlogfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                             \u001b[0mml_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mML_Model_Builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mneur\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                             \u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                             \u001b[0mmodel_chars\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                             \u001b[0mmodel_hists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mml_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-5b5b610b8955>\u001b[0m in \u001b[0;36m_train_model\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         self.history = self.model.fit(data.X_train,data.y_train,epochs=self.epochs,batch_size=self.batch_size,\\\n\u001b[0;32m---> 68\u001b[0;31m                                       validation_data=(data.X_test,data.y_test),verbose=1)\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3738\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3739\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3740\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3742\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1079\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m     \"\"\"\n\u001b[0;32m-> 1081\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1119\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1120\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1121\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/software/pkg/miniconda3/envs/CO2_Eddy/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print_log_flush(\"-------------------------BUILD AND TRAIN MODELS-------------------------\",logfile)\n",
    "\n",
    "    tot_train = len(periods_to_lag)*len(neurons)*len(dropout_rate)*len(learn_rate)*len(decay)*len(batch_size)*len(epochs)-1\n",
    "\n",
    "    model_chars = []\n",
    "    model_hists = []\n",
    "    i = 0\n",
    "\n",
    "    #for lag in periods_to_lag:\n",
    "    ml_data = ML_Data(feature_columns,downsample_sec,periods_to_lag[0],tower,train_percent)\n",
    "    ml_data._ML_Process(data)\n",
    "    for neur in neurons:\n",
    "        for dr in dropout_rate:\n",
    "            for lr in learn_rate:\n",
    "                for dec in decay:\n",
    "                    for bs in batch_size:\n",
    "                        for ep in epochs:\n",
    "                            print_log_flush(f\"---Training Model: {i} of {tot_train}---\",logfile)\n",
    "                            ml_model = ML_Model_Builder(activation,neur,dr,lr,dec,bs,ep)\n",
    "                            ml_model._train_model(ml_data)\n",
    "                            model_chars.append(ml_model.__dict__)\n",
    "                            model_hists.append(ml_model.history.history)\n",
    "\n",
    "                            \n",
    "                            i+=1\n",
    "\n",
    "    error_name  = 'rmse'   \n",
    "\n",
    "    error_vals = []\n",
    "    for m in models:\n",
    "        error_vals.append(m.history.history[error_name][-1])\n",
    "    best_idx = error_vals.index(min(error_vals))\n",
    "\n",
    "    print_log_flush(\"-------------------------RESULTS-------------------------\",logfile)\n",
    "\n",
    "    print_log_flush(f\"Best score for '{error_name}' was {min(error_vals)} in model {best_idx}\",logfile)\n",
    "\n",
    "    print_log_flush(f\"Downsampling Seconds: {models[best_idx].downsample_sec}\\n\\\n",
    "    Lag Periods: {models[best_idx].periods_to_lag}\\n\\\n",
    "    Activation: {models[best_idx].activation}\\n\\\n",
    "    Neurons: {models[best_idx].neurons}\\n\\\n",
    "    Learning Rate: {models[best_idx].learn_rate}\\n\\\n",
    "    Decay: {models[best_idx].decay}\\n\\\n",
    "    Epochs: {models[best_idx].epochs}\",logfile)\n",
    "\n",
    "    print_log_flush(\"-------------------------SAVE FILE-------------------------\",logfile)\n",
    "\n",
    "    import pickle\n",
    "    with open('{}.pkl'.format(file_name), 'wb') as models_file:\n",
    "         pickle.dump(models, models_file)\n",
    "\n",
    "    print_log_flush(f\"Saved list of models to {file_name}\",logfile)\n",
    "    print_log_flush(f\"Models built with optimizer: {models[best_idx].opt_string}\",logfile)\n",
    "\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    print_log_flush('*******************************END TIME = {}************************************'.format(dt_string),logfile)\n",
    "except Exception as e:\n",
    "    print_log_flush('Error occurred ' + str(e),logfile)\n",
    "logfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRIDSEARCH LOAD AND VIEW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from CO2_Dataset_Preparation import *\n",
    "from ML_Model_Master import *\n",
    "\n",
    "with open('ML_Models/models_test.pkl', 'rb') as models_file:\n",
    "    models = pickle.load(models_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "for mod in models:\n",
    "    opt = tf.keras.optimizers.Adam(lr=mod.learn_rate,decay=mod.decay)\n",
    "    mod.model.compile(loss=mod.loss,optimizer=opt,metrics=[rmse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_name  = 'rmse'   \n",
    "\n",
    "error_vals = []\n",
    "for m in models:\n",
    "    error_vals.append(m.history.history[error_name][-1])\n",
    "best_idx = error_vals.index(min(error_vals))\n",
    "\n",
    "print(f\"Best score for '{error_name}' was {min(error_vals)} in model {best_idx}\")\n",
    "print(f\"Downsampling Seconds: {models[best_idx].downsample_sec}\\n\\\n",
    "Lag Periods: {models[best_idx].periods_to_lag}\\n\\\n",
    "Activation: {models[best_idx].activation}\\n\\\n",
    "Neurons: {models[best_idx].neurons}\\n\\\n",
    "Learning Rate: {models[best_idx].learn_rate}\\n\\\n",
    "Decay: {models[best_idx].decay}\\n\\\n",
    "Epochs: {models[best_idx].epochs}\")\n",
    "\n",
    "data = Dataset('../CO2_Data_Final',models[best_idx].position_number)\n",
    "data._preprocess()\n",
    "\n",
    "ml_data = ML_Data(models[best_idx].feature_columns,models[best_idx].downsample_sec\\\n",
    "                      ,models[best_idx].periods_to_lag,models[best_idx].tower,models[best_idx].train_percent)\n",
    "ml_data._ML_Process(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "i=best_idx\n",
    "\n",
    "models[i]._fit_data(ml_data)\n",
    "\n",
    "#Return to original data shape and scale\n",
    "X_test_original_shape = ml_data.X_test.reshape(ml_data.orig_X_test_shape) #reshape from 3d time\n",
    "y_test_original_shape = ml_data.y_test.reshape(ml_data.orig_y_test_shape)#reshape from 3d time \n",
    "\n",
    "merged_tests = np.concatenate((X_test_original_shape,y_test_original_shape[:,None]),axis=1) #concat X and y\n",
    "unscaled_test = pd.DataFrame(ml_data.min_max_scalar.inverse_transform(merged_tests)).iloc[:,-1] #unscale using declared scalar\n",
    "\n",
    "y_fit_original_shape = ml_data.y_fit.reshape(ml_data.orig_y_test_shape)\n",
    "merged_tests = np.concatenate((X_test_original_shape,y_fit_original_shape[:,None]),axis=1)\n",
    "unscaled_fit = pd.DataFrame(ml_data.min_max_scalar.inverse_transform(merged_tests)).iloc[:,-1]\n",
    "\n",
    "#Put into pandas df\n",
    "comparison = pd.concat([unscaled_test,unscaled_fit],axis=1)\n",
    "comparison.columns = ['test','fit']\n",
    "\n",
    "#PLOT PREDICTED VS OBSERVED\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "# ax.plot(ml_data.y_fit,label='ML_Fit_to_Test')\n",
    "# ax.plot(ml_data.y_test,label='Actual_Test_Data')\n",
    "\n",
    "roll = 1\n",
    "\n",
    "ax.plot(comparison['fit'].rolling(roll).mean(),label='ML_Predicted')\n",
    "ax.plot(comparison['test'].rolling(roll).mean(),label='Actual_Test_Data')\n",
    "\n",
    "ax.set_xlabel('Time (s)',fontsize=16)\n",
    "ax.set_ylabel('Mass Flow from Vent (g/s)',fontsize=16)\n",
    "ax.tick_params(axis='both', which='major', labelsize=16)\n",
    "\n",
    "\n",
    "ax.legend(fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (CO2_Eddy)",
   "language": "python",
   "name": "co2_eddy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
